## December 24, 2025: 

`Objective:`
To implement the Computer Science Department Scraper find out if the workflow integrates it well alongside the Data Science department scraper



`Challenges Encountered:`

- (1) I ran into some resistance in scraping from the computer science faculty page. They have a cloudflare screening that prevents bots from scraping the website, containing high standards for what is considered human activity. The standard http request from Beautiful soup (requests.Sessions) consistently returned 403 Forbidden, meaning the page required JavaScript execution and browser level behavior, aka human behavior. Spent a few hours trying to debugging the issue and find another method of scraping
  
- (2)after introducing browser fallback, changing the fetch_page return type for just a str (html) to a tuple (html, fetch_method), there was downstream parsing errors with BeautifulSoup receiving a tuple

- (3)Once I finished all the cs logic, i found that the runtime was really long and that the Playwright browser would open every single time a new url was scraped, which meant the browsers would open and close 90 times. 



`Solution Implemented:`

- (1)Found another html retrieval strategy using the Playwright-based browser which opens its own browser, mimicking a real user and pulling the html. I was thinking of replacing the requests completely with this new retrieval method, but found it to be very computationally expensive and time consuming, especially considering the increase of scale. I instead had the default fetch page method utilize requests, and if there was an error which signaled cloudflare bot protection, then the function would fall back to Playwright. Also implemented another metric for the raw_pages DuckDB table, which will now include method used for scraping (http | browser).

- (2)Fixed the downstream error to only receive the first value of the tuple. Having the underscore in the assignment ignores the second value (which is the scraping method) of the tuple. 
   
        html, _ = BeautifulSoup(html, "html.parser")

- (3) I switched the playwright to use a asynchronous api instead of a sync api, designed to run within an event loop, allowing me to reuse a single playwright browser for many profile pages. The rest of the workflow also remains only synchronous. 

- (4) still taking a long time to run the cs department



`Questions I Have:`

Curious how many departments will have this cloudflare security, and if many whether it will be too computationally expensive. Would I need to move at some point to a virtual machine, also will I make automate the workflow to run ever x days to keep track of changes. Orchestration tools like airflow? containerization in Docker? Also, questioning how I can implement the entire engineering faculty instead of just one department at a time since they have a complete faculty page. There would be slightly more difficulty in the scraping and filtering, but could save time and computational resource when complete. 




`Next Steps:`

Wrap up the Computer Science faculty scraping and move on to Economics and Psychology departments. Also soon reach out to Alex Gates to asses the program structure, the implementation of the Data Science, Computer Science, Economics, and Psychology departments. 