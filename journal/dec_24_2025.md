## December 24, 2025: 

`Objective:`
To implement the Computer Science Department Scraper find out if the workflow integrates it well alongside the Data Science department scraper



`Challenges Encountered:`

- (1) I ran into some resistance in scraping from the computer science faculty page. They have a cloudflare screening that prevents bots from scraping the website, containing high standards for what is considered human activity. The standard http request from Beautiful soup (requests.Sessions) consistently returned 403 Forbidden, meaning the page required JavaScript execution and browser level behavior, aka human behavior. Spent a few hours trying to debugging the issue and find another method of scraping
  
- (2)after introducing browser fallback, changing the fetch_page return type for just a str (html) to a tuple (html, fetch_method), there was downstream parsing errors with BeautifulSoup receiving a tuple

- (3)Once I finished all the cs logic, i found that the runtime was really long and that the Playwright browser would open every single time a new url was scraped, which meant the browsers would open and close 90 times. 

- (4) When scraping the Computer Science department, the program appeared to run indefinitely (over an hour) without completing. During execution, multiple browser windows were repeatedly launched, and progress stalled despite no explicit errors being raised. 




`Solution Implemented:`

- (1)Found another html retrieval strategy using the Playwright-based browser which opens its own browser, mimicking a real user and pulling the html. I was thinking of replacing the requests completely with this new retrieval method, but found it to be very computationally expensive and time consuming, especially considering the increase of scale. I instead had the default fetch page method utilize requests, and if there was an error which signaled cloudflare bot protection, then the function would fall back to Playwright. Also implemented another metric for the raw_pages DuckDB table, which will now include method used for scraping (http | browser).

- (2)Fixed the downstream error to only receive the first value of the tuple. Having the underscore in the assignment ignores the second value (which is the scraping method) of the tuple. 
   
        html, _ = BeautifulSoup(html, "html.parser")

- (3) I switched the playwright to use a asynchronous api instead of a sync api, designed to run within an event loop, allowing me to reuse a single playwright browser for many profile pages. The rest of the workflow also remains only synchronous. 

- (4) The issue was caused by an incorrect interaction between synchronous and asynchronous code. The page-fetching logic was converted to async, but several calling functions still treated it as synchronous. As a result, coroutine objects were repeatedly created but never awaited. This caused the event loop to schedule unresolved tasks indefinitely while higher-level loops continued executing, leading to infinite looping. I made it so that all functions involved in page discovery and fetching were consistently asynchronous, specifically that:

        - the `get_faculty_links()`  was converted to an async method  
        - all calls to `fetch_page()` were properly awaited
        - the browser creation was centralized and reused across pages instead of being repeatedly initialized
        - cleanup was handled with `await` instead of with nested event loops
After this I was able to run my scraper in a loop utilizing both the Data science scraper and the Computer Science scraper.


`Questions I Have:`

Curious how many departments will have this cloudflare security, and if many whether it will be too computationally expensive. Would I need to move at some point to a virtual machine, also will I make automate the workflow to run ever x days to keep track of changes. Orchestration tools like airflow? containerization in Docker? Also, questioning how I can implement the entire engineering faculty instead of just one department at a time since they have a complete faculty page. There would be slightly more difficulty in the scraping and filtering, but could save time and computational resource when complete. 




`Next Steps:`

Wrap up the Computer Science faculty scraping and move on to Economics and Psychology departments which will likely be easier to implement since they likely won't have the cloudflare issues that the cs department had, though if it did, my code can now handle those. Also soon reach out to Alex Gates to asses the program structure, the implementation of the Data Science, Computer Science, Economics, and Psychology departments. 